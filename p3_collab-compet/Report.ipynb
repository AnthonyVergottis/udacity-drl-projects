{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3  - Tennis\n",
    "Written by: [Anthony Vergottis](http://github.com/anthonyvergottis) \n",
    "\n",
    "In this environment there are two agents, each controlling a racket, with the goal of hitting the ball over the net. If the agent hits the ball over the net it received a reward of 0.1, if it lets the ball hit the ground it receives a reward of -0.01. Therefore, the objective for each agent is to keep the ball in play.\n",
    "\n",
    "Each agents observation consists of 8 variables, corresponding to the position and velocity of both the ball and the racket. Each agent is not aware of the existence of an other agent. The action space for each agent is two continuous values, moving towards and away from the net, and moving up and down. The environment is set up to receive three stacked observations, this results in an overall state observation of size 24.\n",
    "\n",
    "Each task is episodic in nature. The goal is for both agents to get an average score greater or equal to 0.5 over 100 consecutive episodes (after taking the maximum for both agents).\n",
    "\n",
    "There are multiple ways in which this problem can be solved. Unfortunately, I was not able to get the MADDPG algorithm to work, it simply would not learn, I could not understand why.\n",
    "\n",
    "Instead the same approach was used as in the second project, The Reacher Arm, but was extended to work with two agents. One actor-critic network was initialised and trained using the experience gathered from both agents (both agents added experience to the same reply buffer). It was trained 10 times every four time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report is split in three parts:\n",
    "\n",
    "1. **Learning algorithm**\n",
    "2. **Plot of rewards**\n",
    "3. **Ideas for future work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm used to solve this problem was the same as in this [paper](https://arxiv.org/abs/1509.02971). It is a deep deterministic policy gradient algorithm (DDPG) that is capable of working with continuous actions spaces. For this case it was extended to work with two agents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values are named as they are found in the code:\n",
    "\n",
    "    - BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    - BATCH_SIZE = 256        # minibatch size\n",
    "    - GAMMA = 0.99            # discount factor\n",
    "    - TAU = 0.2              # for soft update of target parameters\n",
    "    - LR_ACTOR = 1e-4         # learning rate actor\n",
    "    - LR_CRITIC = 1e-3        # learning rate critic\n",
    "    - n_episodes = 2000       # Limit of number of episodes to run\n",
    "    - max_t = 2000            # Max No. of time steps per episode\n",
    "    - WEIGHT_DECAY = 0        # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some alterations were made from the original paper in order to improve performance of the algorithm.\n",
    "\n",
    "1. Instead of using 400 and 300 units in the first and second layer of the network, a new network architecture of 128 units in both layers was used.\n",
    "\n",
    "2. Batch normalization was used after the first layer in both the actor and critic networks.\n",
    "\n",
    "3. The L2 weight decay for the critic network was set to 0.\n",
    "\n",
    "4. The gradients were clipped in the critic network. This suggestion was found in the benchmark implementation. Using torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1).\n",
    "\n",
    "5. Instead of using a uniform distribution for the Ornstein-Uhlenbeck process a normal distribution was used. This yielded much better results.\n",
    "\n",
    "6. It was suggested by one of my colleges to increase the value of tau to 0.2 as it would result in faster learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DDPG agent solved the environment in 664 episodes, with an Average Score: 0.5\n",
    "![ddpg_scoress.png](images/ddpg_scoress.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "1. Implement the PPO algorithm, the DDPG algorithm took rather long to solve the environment.\n",
    "2. Try using prioritized replay memory\n",
    "3. Try adding noise to the policy parameters\n",
    "4. Experiment with different network weight initialisation\n",
    "5. Implement Leaky ReLU activations in network rather RelU\n",
    "6. Further hyperparameter tuning. Given the slow nature of the DDPG algorithm in this case, did not allow for correct exploration.\n",
    "7. The results appear to be rather noisy. Further exploration to find out the cause would be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
