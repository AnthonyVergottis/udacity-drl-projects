{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Reacher Arm\n",
    "Written by: [Anthony Vergottis](http://github.com/anthonyvergottis) \n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1. The simulation environment is provided by Unity ML.\n",
    "\n",
    "The single agent version of the environment was used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report is split in three parts:\n",
    "\n",
    "1. **Learning algorithm**\n",
    "2. **Plot of rewards**\n",
    "3. **Ideas for future work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm used to solve this problem was the same as in this [paper](https://arxiv.org/abs/1509.02971). It is a deep deterministic policy gradient algorithm (DDPG) that is capable of working with continuous actions spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values are named as they are found in the code:\n",
    "\n",
    "    - BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    - BATCH_SIZE = 128        # minibatch size\n",
    "    - GAMMA = 0.99            # discount factor\n",
    "    - TAU = 1e-3              # for soft update of target parameters\n",
    "    - LR_ACTOR = 1e-4         # learning rate actor\n",
    "    - LR_CRITIC = 1e-3        # learning rate critic\n",
    "    - n_episodes = 2000       # Limit of number of episodes to run\n",
    "    - max_t = 1000            # Max No. of time steps per episode\n",
    "    - WEIGHT_DECAY = 0        # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some alterations were made from the original paper in order to improve performance of the algorithm.\n",
    "\n",
    "1. Instead of using 400 and 300 units in the first and second layer of the network, a new network architecture of 128 units in both layers was used.\n",
    "\n",
    "2. Batch normalization was used after the first layer in both the actor and critic networks.\n",
    "\n",
    "3. The L2 weight decay for the critic network was set to 0.\n",
    "\n",
    "4. The gradients were clipped in the critic network. This suggestion was found in the benchmark implementation. Using torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1).\n",
    "\n",
    "5. Instead of using a uniform distribution for the Ornstein-Uhlenbeck process a normal distribution was used. This yielded much better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DDPG agent solved the environment in 449 episodes, with an Average Score: 13.03\n",
    "![dqn_scores.png](images/ddpg_scores.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "1. Implement the PPO algorithm, the DDPG algorithm took rather long to solve the environment.\n",
    "2. Try using prioritized replay memory\n",
    "3. Try adding noise to the policy parameters\n",
    "4. Experiment with different network weight initialisation\n",
    "5. Implement Leaky ReLU activations in network rather RelU\n",
    "6. Further hyperparameter tuning. Given the slow nature of the DDPG algorithm in this case, did not allow for correct exploration.\n",
    "7. The results appear to be rather noisy. Further exploration to find out the cause would be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
